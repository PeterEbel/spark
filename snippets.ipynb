{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a07e12f-c51a-4c20-a13c-a7f07250212e",
   "metadata": {},
   "source": [
    "<h1><b>Important Spark Concepts</b></h1>\n",
    "In Apache Spark, both <b>SparkSession</b> and <b>SparkContext</b> are important components, but they serve different purposes and are used in different contexts.\n",
    "\n",
    "<h2>SparkContext</h2>\n",
    "<ul>\n",
    "<li>SparkContext is the entry point to Spark and represents the connection to a Spark cluster. It is the core component of Spark and is responsible for coordinating the execution of tasks in a cluster.</li>\n",
    "<li>It is primarily used for low-level operations such as creating Resilient Distributed Datasets (RDDs), which are the fundamental data structure in Spark.</li>\n",
    "<li>SparkContext is not aware of structured data like DataFrames, and it does not provide a high-level API for structured data processing.</li>\n",
    "</ul>\n",
    "<h2>SparkSession</h2>\n",
    "<ul>\n",
    "<li>SparkSession was introduced in Spark 2.0 and is an abstraction built on top of the SparkContext. It's designed for higher-level, more user-friendly structured data processing, including working with DataFrames and Datasets.</li>\n",
    "<li>SparkSession is a unified entry point for working with structured data in Spark. It provides a convenient API for creating, manipulating, and querying structured data.</li>\n",
    "<li>It includes functionalities for working with structured data sources like Parquet, Avro, ORC, JSON, and more, and it also allows you to interact with structured data using SQL queries via the Spark SQL module.</li>\n",
    "</ul>\n",
    "In summary, the key difference between SparkContext and SparkSession is their purpose and level of abstraction:\n",
    "\n",
    "<ul>\n",
    "<li><b>SparkContext</b> is the fundamental entry point for Spark and is primarily used for low-level operations and RDD-based data processing.</li>\n",
    "<li><b>SparkSession</b> is a higher-level, more user-friendly entry point for structured data processing, including DataFrames, Datasets, and Spark SQL.</li>\n",
    "</ul>\n",
    "    In practice, if you are working with structured data, it is recommended to use SparkSession for its ease of use and more powerful capabilities. If you are working with older Spark code or low-level operations, you may still encounter SparkContext, but modern Spark applications often use SparkSession for structured data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de36aad-c2c8-40c4-85af-20671df0d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession und SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Introduction to Spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d01a0-35a9-4f1a-a82b-f24047faf15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playing around with JSON file and DataFrames\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Read a JSON File\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"multiline\",\"true\").json(\"/home/peter/Projects/spark/data/customers.json\")\n",
    "df.printSchema()\n",
    "\n",
    "# Display some columns of the firts 10 DataFrame records\n",
    "df.select('last_name', 'first_name').show(10)\n",
    "\n",
    "# Display females only\n",
    "df.select('last_name', 'first_name', 'gender_code').filter(df['gender_code'] == 'F').show(10)\n",
    "\n",
    "# Group by gender\n",
    "df.groupBy(\"gender_code\").count().show()\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM customers where first_name =='Peter'\")\n",
    "sqlDF.select('first_name', 'last_name', 'birth_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45a5f8-afe1-4908-a78f-a76dfa04e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"pi\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ad192-08f7-4992-998e-02a899c4a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "print(random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79f95d-6a77-400d-b62b-178f958c3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first contact with Spark\n",
    "# Aggregations - grouping. average, min, max\n",
    "\n",
    "# import non-standard functions\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Dataframes and aggretations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# create a dataframe with named columns from an array of tuples (corresponds to a table with 2 columns and 6 rows) \n",
    "person_df = spark.createDataFrame([(\"Peter\", 20), (\"Thomas\", 31), (\"Michael\", 30), (\"Anabel\", 35), (\"Elke\", 25), (\"Peter\", 58)], [\"name\", \"age\"])\n",
    "\n",
    "# group person with the same name, aggregate over ages and calculate avg age\n",
    "avg_age_df = person_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "\n",
    "# show results\n",
    "person_df.show()\n",
    "avg_age_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f0204-44f3-4155-a51f-9ddadb318a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data in CSV files \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"CSV\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .load(\"/home/peter/Projects/spark/data/customers.csv\")\n",
    "\n",
    "# number of records in a dataframe\n",
    "# print(df.count())\n",
    "\n",
    "# show first 10 records of a dataframe\n",
    "# df.limit(10).show()\n",
    "\n",
    "# show selected columns only\n",
    "# df.select('last_name', 'first_name', 'gender_code').limit(10).show()\n",
    "\n",
    "# filter by gender code\n",
    "df.select('last_name', 'first_name', 'gender_code').filter(\"gender_code = 'F'\").limit(10).show()\n",
    "# df.filter(\"gender_code = 'F'\").show()\n",
    "\n",
    "# write file with female entries only\n",
    "# df_female_customers = df.filter(\"gender_code = 'F'\")\n",
    "# df_female_customers.limit(10).show()\n",
    "# df_female_customers.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"|\").save(\"dbfs:/user/n68563/bdfb/female_customers\")\n",
    "# df_female_customers.coalesce(1).write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"|\").save(\"dbfs:/user/n68563/bdfb/test.csv\")\n",
    "# df2 = spark.read.format(\"csv\").option(\"inferSchema\", \"false\").option(\"header\", \"false\").option(\"sep\", \"|\").load(\"/user/n68563/bdfb/female_customers\")\n",
    "# df2.limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f000af6-2d67-4be6-914f-ff13b5da764f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/13 11:40:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------+\n",
      "|  last_name|first_name|data_date_part|\n",
      "+-----------+----------+--------------+\n",
      "|       Rose|    Rosita|    2021-11-08|\n",
      "|       Junk|    Hatice|    2021-11-08|\n",
      "|     Biggen|  Virginia|    2021-11-08|\n",
      "|      Gnatz|      Ella|    2021-11-08|\n",
      "|Wagenknecht|  Reinhild|    2021-11-08|\n",
      "|    Hauffer|  Birgitta|    2021-11-08|\n",
      "|    Gerlach|    Sabina|    2021-11-08|\n",
      "|  Striebitz|   Hiltrud|    2021-11-08|\n",
      "|   Fröhlich|   Evelyne|    2021-11-08|\n",
      "|     Hiller|   Damaris|    2021-11-08|\n",
      "|     Johann|  Nadeshda|    2021-11-08|\n",
      "|     Ladeck|  Ljiljana|    2021-11-08|\n",
      "|      Klapp|  Fabienne|    2021-11-08|\n",
      "|   Schuster|      Änne|    2021-11-08|\n",
      "|      Kambs|     Henni|    2021-11-08|\n",
      "|       Hahn|  Nadeshda|    2021-11-08|\n",
      "| Vollbrecht|    Zdenka|    2021-11-08|\n",
      "|    Ziegert|   Martina|    2021-11-08|\n",
      "|    Radisch|      Leni|    2021-11-08|\n",
      "|     Weimer|   Henrike|    2021-11-08|\n",
      "+-----------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+----------+--------------+\n",
      "|  last_name|first_name|data_date_part|\n",
      "+-----------+----------+--------------+\n",
      "|       Rose|    Rosita|    2023-10-20|\n",
      "|       Junk|    Hatice|    2023-10-20|\n",
      "|     Biggen|  Virginia|    2023-10-20|\n",
      "|      Gnatz|      Ella|    2023-10-20|\n",
      "|Wagenknecht|  Reinhild|    2023-10-20|\n",
      "|    Hauffer|  Birgitta|    2023-10-20|\n",
      "|    Gerlach|    Sabina|    2023-10-20|\n",
      "|  Striebitz|   Hiltrud|    2023-10-20|\n",
      "|   Fröhlich|   Evelyne|    2023-10-20|\n",
      "|     Hiller|   Damaris|    2023-10-20|\n",
      "|     Johann|  Nadeshda|    2023-10-20|\n",
      "|     Ladeck|  Ljiljana|    2023-10-20|\n",
      "|      Klapp|  Fabienne|    2023-10-20|\n",
      "|   Schuster|      Änne|    2023-10-20|\n",
      "|      Kambs|     Henni|    2023-10-20|\n",
      "|       Hahn|  Nadeshda|    2023-10-20|\n",
      "| Vollbrecht|    Zdenka|    2023-10-20|\n",
      "|    Ziegert|   Martina|    2023-10-20|\n",
      "|    Radisch|      Leni|    2023-10-20|\n",
      "|     Weimer|   Henrike|    2023-10-20|\n",
      "+-----------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apache Iceberg\n",
    "\n",
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"Iceberg\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/home/peter/Projects/spark/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_customers = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/home/peter/Projects/spark/data/customers.csv\")\n",
    "\n",
    "df_customers.select('last_name', 'first_name', 'gender_code', 'data_date_part').filter(\"gender_code = 'F'\").createOrReplaceTempView(\"temp_view_customers\")\n",
    "spark.sql(\"CREATE or REPLACE TABLE local.db.customers USING iceberg AS SELECT * FROM temp_view_customers\")\n",
    "df = spark.sql(\"SELECT last_name, first_name, data_date_part FROM local.db.customers\")\n",
    "df.show()\n",
    "df = spark.sql(\"UPDATE local.db.customers SET data_date_part = '2023-10-20' WHERE data_date_part = '2021-11-08'\")\n",
    "df = spark.sql(\"SELECT last_name, first_name, data_date_part FROM local.db.customers\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c19a7e-42f2-4781-9507-c232d2b16a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a function to read from a MariaDB and from PostgreSQL database table\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Databases\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark-3.5.0/jars/postgresql-42.6.0.jar\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark-3.5.0/jars/mariadb-java-client-3.2.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def show_customers(spark: SparkSession, database) -> None:\n",
    "    if (database.lower() == \"mariadb\"):\n",
    "        df_customers = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:mysql://localhost:3306/shop?permitMysqlScheme\") \\\n",
    "            .option(\"driver\", \"org.mariadb.jdbc.Driver\") \\\n",
    "            .option(\"dbtable\", \"customers\") \\\n",
    "            .option(\"user\", \"spark\") \\\n",
    "            .option(\"password\", \"spark\") \\\n",
    "            .load()\n",
    "        df_customers.select('last_name', 'first_name', 'birth_date').show(10)\n",
    "    elif (database.lower() == \"postgresql\"):\n",
    "        df_customers = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:postgresql://localhost:5432/galeria_anatomica\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .option(\"dbtable\", \"shop.customers\") \\\n",
    "            .option(\"user\", \"spark\") \\\n",
    "            .option(\"password\", \"spark\") \\\n",
    "            .load()\n",
    "        df_customers.select('last_name', 'first_name', 'birth_date').show(10)\n",
    "    else:\n",
    "        print(\"Unbekannter Datenbanktyp\")\n",
    "\n",
    "show_customers(spark, \"postgresql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123ce14-b78e-4bb6-93df-94142db71fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and read Parquet files\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Write and read a Parquet file\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "customers_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \"|\").load(\"/home/peter/Projects/spark/data/customers.csv\")\n",
    "customers_df.write.mode(\"overwrite\").parquet(\"/home/peter/Projects/spark/data/customers.parquet\")\n",
    "parquet_df = spark.read.parquet(\"/home/peter/Projects/spark/data/customers.parquet\")\n",
    "parquet_df.createOrReplaceTempView(\"parquet_file\")\n",
    "males_df = spark.sql(\"SELECT last_name, first_name, gender_code FROM parquet_file WHERE gender_code = 'M'\")\n",
    "males_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b710a5e2-7500-4fe7-b3f8-adbf45cb45cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
