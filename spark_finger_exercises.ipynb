{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a07e12f-c51a-4c20-a13c-a7f07250212e",
   "metadata": {},
   "source": [
    "# Introduction into Apache Spark Programming\n",
    "---\n",
    "__Santander Consumer Bank Germany__  \n",
    "__CTO/Architecture__  \n",
    "\n",
    "__Version:__ 1.0  \n",
    "__Date:__ 2023-10-19  \n",
    "__Github:__\n",
    "\n",
    "This document is intended to serve as a first introduction to programming Apache Spark using the Python framework PySpark. Basic knowledge of the Python programming language is required. The most important concepts and data structures of Spark, in particular Resilient Distributed Datasets (RDD) and DataFrames, are presented using short example programs. Jupyter Notebook and Microsoft Visual Code are used as development environments. Many of the examples are taken from the [official Spark documentation](https://spark.apache.org/docs/latest/index.html) and some have been slightly modified.\n",
    "\n",
    "## What is Apache Spark?\n",
    "\n",
    "Apache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.\n",
    "\n",
    "## Installation\n",
    "\n",
    "There are mainly three options to run Apache Spark and Jupyter. You can use a ready-to-go Docker image, setup a Linux environment in a Virtual Machine and install Spark and Jupyter or follow a procedure that installs Spark and Jupyter on your Windows system.\n",
    "\n",
    "### Docker\n",
    "The eastiest and quickest way to use Spark and Jupyter is Docker. A Docker image containing Jupyter and the Spark stack is available at [https://hub.docker.com/r/jupyter/pyspark-notebook](https://hub.docker.com/r/jupyter/pyspark-notebook) and can be pulled from the Dockerhub repositories. With Docker installed on your system execute the following command:\n",
    "\n",
    "```bash\n",
    "docker pull jupyter/pyspark-notebook\n",
    "```\n",
    "The image is about 1,7 GB so the download and might take some time depending on your Internet connection. Once the image is downloaded and installed in your local Docker repository you can start the container with\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 8888:8888 -v /home/peter/Projects/spark:/home/jovyan/work jupyter/pyspark-notebook\n",
    "\n",
    "```\n",
    "I have used the -v switch which allows me to persist the data that I might generate in the Jupyter Notebook of the Docker container on my local hard disc (Docker containers can't persist data per se). To do so I've mapped my local host folder (/home/peter/Projects/spark) to the default folder of Jupyter Notebook in the Docker container, which is “/home/jovyan/work”. In case you don't need to persist data just execute the _run_ command without -v:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 8888:8888 jupyter/pyspark-notebook\n",
    "\n",
    "```\n",
    "\n",
    "![URL](jupyter_in_docker.jpg)\n",
    "\n",
    "When the startup process is finished copy the URL that I've marked with a red line in the picture above and paste it into your preferred browser. After a couple of seconds the Jupyter Notebook should appear and you are ready start programming Spark.\n",
    "\n",
    "### Linux Virtual Machine\n",
    "\n",
    "The installation of Apache Spark and Jupyter that is described below was carried out on the Debian-based Linux distribution Xubuntu 22.04.3 LTS. If you want to play around with Spark on Linux you can run a virtual machine using the free [VMWare Player](https://www.vmware.com/products/workstation-player.html) and use a [Xubuntu ISO image](https://xubuntu.org/download/) for installation. The setup procedure is only slightly more complex than the Docker option. \n",
    "\n",
    "As a Linux user with sudo permissions execute the following commands:\n",
    "\n",
    "```bash\n",
    "sudo apt-get update\n",
    "sudo apt install openjdk-19-jdk openjdk-19-jre\n",
    "wget https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
    "tar -zxvf spark-3.5.0-bin-hadoop3.tgz\n",
    "sudo mv spark-3.5.0-hadoop3 /opt\n",
    "```\n",
    "\n",
    "Add the following lines to the file /etc/profiles to publish some environment variables:<br>\n",
    "\n",
    "```bash\n",
    "export SPARK_HOME=/opt/spark-3.5.0-bin-hadoop3\n",
    "export PATH=\\\\$PATH:\\\\$SPARK_HOME/bin:\\\\$SPARK_HOME/sbin\n",
    "export PYSPARK_PYTHON=/usr/bin/python3\n",
    "export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "```\n",
    "\n",
    "Save the file. Back on shell source the profile file:<br>\n",
    "\n",
    "```bash\n",
    "source /etc/profile\n",
    "```\n",
    "\n",
    "Change into your project folder, create and activate a virtual environment and install PySpark and Jupyter:<br>\n",
    "\n",
    "```bash\n",
    "cd /path/to/my/project/folder\n",
    "python -m venv ./venv\n",
    ". ./venv/bin/activate\n",
    "pip install pyspark\n",
    "pip install jupyter\n",
    "```\n",
    "Now start Juypther Notebooks:\n",
    "\n",
    "```bash\n",
    "pyspark\n",
    "```\n",
    "\n",
    "### PySpark and Jupyter on Windows\n",
    "\n",
    "Installing PySpark on Windows and the use of Visual Code for programming is described very well in the following YouTube video: [How to run PySpark in Visual Studio Code (on Windows)](https://www.youtube.com/results?search_query=how+to+run+pyspark+in+visual+studio+code). Download and install Microsoft Visual Code and follow the instructions shown in the video. To install Jupyter and integrate it with Spark check this out: [Get Started with PySpark and Jupyter Notebook in 3 Minutes](https://medium.com/sicara/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6ImM2MjYzZDA5NzQ1YjUwMzJlNTdmYTZlMWQwNDFiNzdhNTQwNjZkYmQiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiIyMTYyOTYwMzU4MzQtazFrNnFlMDYwczJ0cDJhMmphbTRsamRjbXMwMHN0dGcuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDA3ODAxOTg4MzMwNTI2NDU2MzMiLCJlbWFpbCI6InBldGVyLmViZWwuanJAZ29vZ2xlbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwibmJmIjoxNjk3MTAxMzIyLCJuYW1lIjoiUGV0ZXIgRWJlbCIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BQ2c4b2NJRVBQeHlmeXJ6MFRCQzI5OXl0LUJ6RS1fN2htMDY1cDNNeXRuYjkwMXI9czk2LWMiLCJnaXZlbl9uYW1lIjoiUGV0ZXIiLCJmYW1pbHlfbmFtZSI6IkViZWwiLCJsb2NhbGUiOiJkZSIsImlhdCI6MTY5NzEwMTYyMiwiZXhwIjoxNjk3MTA1MjIyLCJqdGkiOiJiN2JkNGYyZTAwNDEyZjAyYjMxYjUzNmJkY2E0NWE3NzI5YTNiYzBiIn0.royHv8iBfsa4UYr-EyKjtQKCkUyIVuUn-KqsxPzW8rGneRtgQxm0i_pZEEnA9Gx6BbR99mFTNro2u9AMVKS-apgUfxSmcZe3AODY6g3TzsuC1wMyA02JulJIfPzXdsPKfrgYEtGAofL7A4_isrvlOWvNEHLDn2ZeidwZVAtoKzb4PYkkeLj6wRWYpmCESJUWW3gsKNEj5UoJiPV7no8nGB4fMEzLFOCL_LoCOqLP4f4Ig5fv_HM68dU5Exzpl4xpzLUBtmQNiEOeuBu3bZoaN30AdbDqkIF-4kgld18_Pp8MSGoWNpDhX4lXYKp4QU1G-xM0Kq7qDcntXP6rLAjmHw)\n",
    "\n",
    "## Important concepts\n",
    "In Apache Spark, both __SparkSession__ and __SparkContext__ are important components, but they serve different purposes and are used in different contexts.\n",
    "\n",
    "[SparkSession](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala) source code (Scala) at Github  \n",
    "[SparkContext](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkContext.scala) source code (Scala) at Github\n",
    "\n",
    "### SparkContext\n",
    "* _SparkContext_ is the entry point to Spark and represents the connection to a Spark cluster. It is the core component of Spark and is responsible for coordinating the execution of tasks in a cluster.\n",
    "* It is primarily used for low-level operations such as creating Resilient Distributed Datasets (RDDs), which are the fundamental data structure in Spark.\n",
    "* _SparkContext_ is not aware of structured data like DataFrames, and it does not provide a high-level API for structured data processing.\n",
    "\n",
    "### SparkSession\n",
    "* _SparkSession_ was introduced in Spark 2.0 and is an abstraction built on top of the _SparkContext_. It's designed for higher-level, more user-friendly structured data processing, including working with DataFrames and Datasets.\n",
    "* _SparkSession_ is a unified entry point for working with structured data in Spark. It provides a convenient API for creating, manipulating, and querying structured data.\n",
    "* It includes functionalities for working with structured data sources like Parquet, Avro, ORC, JSON, and more, and it also allows you to interact with structured data using SQL queries via the Spark SQL module.\n",
    "\n",
    "In summary, the key difference between _SparkContext_ and _SparkSession_ is their purpose and level of abstraction: _SparkContext_ is the fundamental entry point for Spark and is primarily used for low-level operations and RDD-based data processing whereas _SparkSession_ is a higher-level, more user-friendly entry point for structured data processing, including DataFrames and Spark SQL.\n",
    "\n",
    "<style>\n",
    "mark {\n",
    "    color:red;\n",
    "}\n",
    "</style>\n",
    "\n",
    "In practice, if you are working with structured data, it is recommended to use SparkSession for its ease of use and more powerful capabilities. If you are working with older Spark code or low-level operations, you may still encounter SparkContext, but modern Spark applications often use SparkSession for structured data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de36aad-c2c8-40c4-85af-20671df0d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession und SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Introduction to Spark\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(spark)\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d432e43-1515-4f3d-aa18-e0dbde86244c",
   "metadata": {},
   "source": [
    "# RDDs and DataFrames\n",
    "\n",
    "[RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)  \n",
    "[SQL Programming Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)  \n",
    "[Getting Started - PySpark](https://spark.apache.org/docs/latest/api/python/getting_started/index.html)\n",
    "\n",
    "## What are RDDs?\n",
    "A __RDD__, or Resilient Distributed Dataset, is a fundamental data structure in Apache Spark. RDDs are designed to provide an efficient and fault-tolerant way to distribute and process data across a cluster of computers. They have several key characteristics:\n",
    "\n",
    "* __Resilient:__ RDDs are \"resilient\" because they can recover from node failures. If a node in the cluster fails, the data and the operations applied to it can be reconstructed on another node, ensuring fault tolerance.\n",
    "* __Distributed:__ RDDs are distributed across multiple nodes in a cluster, allowing for parallel processing of data. This distribution is a fundamental feature of Spark, making it suitable for handling large datasets.\n",
    "* __Immutable:__ Once created, RDDs are immutable, meaning their data cannot be changed. Instead, transformations applied to an RDD result in the creation of new RDDs, which helps maintain data consistency and enables lineage information for fault recovery.\n",
    "* __In-Memory:__ RDDs are typically stored in memory, which makes them faster to access and process compared to reading and writing to disk. This in-memory storage is one of the key performance benefits of Spark.\n",
    "* __Lazy execution:__ When Spark transforms data, it does not immediately compute the transformation but plans how to compute later. When actions such as collect() are explicitly called, the computation starts.\n",
    "\n",
    "Common operations on RDDs include mapping, filtering, reducing, and joining data.\n",
    "\n",
    "While RDDs were the primary data abstraction in earlier versions of Spark, more recent versions introduced __DataFrames__ and Datasets, which provide higher-level abstractions and optimizations for structured data processing. Nevertheless, RDDs still play a crucial role in Spark, particularly when you need fine-grained control over data and operations or when working with unstructured data.\n",
    "\n",
    "## What are DataFrames?\n",
    "Spark __DataFrames__ are a distributed collection of data organized into named columns, much like a table in a relational database or a data frame in R or Python. They are a higher-level abstraction built on top of Resilient Distributed Datasets (RDDs) in Apache Spark, designed to provide a more structured and optimized way to work with data. DataFrames were introduced in Spark 2.0 to address the limitations of RDDs, particularly when dealing with structured and semi-structured data.\n",
    "\n",
    "Key features and characteristics of Spark DataFrames include:\n",
    "* __Schema:__ DataFrames have a well-defined schema that describes the structure of the data, including column names and data types. This schema information helps Spark optimize query execution and provides type safety.\n",
    "* __Performance:__ DataFrames are designed for optimized performance. Spark can leverage the schema information to perform predicate pushdown, column pruning, and other query optimizations to speed up data processing.\n",
    "* __Language support:__ DataFrames are available in multiple programming languages, including Scala, Java, Python, and R. This means you can work with DataFrames in a language you're comfortable with.\n",
    "* __Interoperability:__ DataFrames can seamlessly interoperate with RDDs, allowing you to use the right data abstraction for your specific use case.\n",
    "* __Catalyst query optimizer:__ Spark DataFrames use the Catalyst query optimizer, which is a powerful tool for optimizing query plans. Catalyst can apply a wide range of optimizations, making queries more efficient.\n",
    "* __Tungsten execution engine:__ DataFrames also use the Tungsten execution engine, which is designed for code generation and in-memory processing. This engine further enhances performance.\n",
    "* __Data sources:__ DataFrames can read and write data from various sources, including Parquet, Avro, ORC, JSON, CSV, and more, making it easy to work with a variety of data formats.\n",
    "* __SQL support:__ Spark DataFrames support SQL queries, enabling you to write SQL-like statements for data manipulation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a37bcb-0f57-4913-8ece-d4be55306d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDDs\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDDs\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd_numbers = sc.parallelize(range(1, 11))\n",
    "# altenatively: rdd_numbers = spark.sparkContext.parallelize(range(1, 11))\n",
    "\n",
    "list_numbers = rdd_numbers.collect()\n",
    "# print(list_numbers[5])\n",
    "\n",
    "for n in list_numbers:\n",
    "   print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa58be4e-0e04-48ad-b7a4-48a79dfc9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map and Reduce\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Map and Reduce\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd_squared_numbers = sc.parallelize(range(1, 11)).map(lambda n: n**2)\n",
    "# altenatively: rdd_numbers = spark.sparkContext.parallelize(range(1, 11))\n",
    "\n",
    "list_numbers = rdd_squared_numbers.collect()\n",
    "\n",
    "print(\"The first 10 square numbers are:\")\n",
    "print()\n",
    "\n",
    "for n in list_numbers:\n",
    "   print(n)\n",
    "print()    \n",
    "\n",
    "sum = sc.parallelize(range(1, 11)).map(lambda n: n**2).reduce(add)\n",
    "print(\"The Sum of the first 10 square numbers is {summe}\".format(summe = sum))\n",
    "print()\n",
    "\n",
    "rdd = sc.parallelize([(\"Peter\", 10), (\"Thomas\", 10), (\"Peter\", 20), (\"Thomas\", 30)])\n",
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1697196-b538-45b6-aed6-c4f0c6484ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Filter\").getOrCreate() \n",
    "\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "rdd.filter(lambda x: x % 2 == 0).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2382d9b1-bed0-4a82-814a-076dd9eb7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing functions as parameters\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder.appName(\"Passing functions as parameters\").getOrCreate() \n",
    "\n",
    "def myFunction(s):\n",
    "    words = s.split(\" \")\n",
    "    return len(words)\n",
    "\n",
    "rdd_words = spark.sparkContext.textFile(\"/home/peter/Projects/spark/data/words.txt\").map(myFunction)\n",
    "number_of_words_in_file = rdd_words.collect()[0]\n",
    "\n",
    "print(\"The file contains {count} words.\".format(count = number_of_words_in_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd45a5f8-afe1-4908-a78f-a76dfa04e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pi\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ae107f-5c29-4e5d-813b-79d610dd416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an RDD from a file\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder.appName(\"RDD from a file\").getOrCreate() \n",
    "\n",
    "rdd_data = spark.sparkContext.textFile(\"/home/peter/Projects/spark/data/data.txt\")\n",
    "print(rdd_data.collect())\n",
    "lineLengths = rdd_data.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a821cd-63f2-4145-8607-825dcef4a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small text files\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder.appName(\"Small text files\").getOrCreate() \n",
    "\n",
    "# Load a text file and convert each line to a Row\n",
    "rdd_textfiles = spark.sparkContext.wholeTextFiles(\"/home/peter/Projects/spark/data/small-text-files\")\n",
    "\n",
    "list_textfiles = rdd_textfiles.sortByKey().collect()\n",
    "\n",
    "for tf in list_textfiles:\n",
    "    print(tf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6b60f-9f8b-4663-93dc-818840ac6fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD finger exercises\n",
    "\n",
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder.appName(\"RDDs\").getOrCreate() \n",
    "\n",
    "# Create RDD from parallelize    \n",
    "list_person = [(\"Peter\", 60), (\"Thomas\", 59), (\"Michael\", 54), (\"Anabel\", 50)]\n",
    "rdd_person = spark.sparkContext.parallelize(list_person)\n",
    "rdd_person.setName('Personen')\n",
    "\n",
    "print(\"The RDD {rdd_name} contains {anzahl} records.\".format(rdd_name = rdd_person.name(), anzahl = rdd_person.count()))\n",
    "print()\n",
    "\n",
    "for p in rdd_person.collect():\n",
    "  # Print name only\n",
    "    print(p[0])\n",
    "print()\n",
    "    \n",
    "print(\"First record before sorting: {ds}\".format(ds = rdd_person.first()))\n",
    "rdd_person_sorted = rdd_person.sortByKey()\n",
    "print(\"First record after sorting:  {ds}\".format(ds = rdd_person_sorted.first()))\n",
    "print()\n",
    "\n",
    "# Filter \n",
    "rdd_numbers = spark.sparkContext.parallelize(range(1, 10 + 1))\n",
    "print(rdd_numbers.collect())\n",
    "print(rdd_numbers.filter(lambda x: x % 2 == 0).collect())\n",
    "print()\n",
    "\n",
    "# Cartesian product \n",
    "rdd1 = spark.sparkContext.parallelize(range(1, 10 + 1))\n",
    "rdd2 = spark.sparkContext.parallelize(range(1, 10 + 1))\n",
    "print(rdd1.cartesian(rdd2).collect())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb231c93-5247-4ea3-8d19-4895b63a6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame from a list of tuples\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes\").getOrCreate()\n",
    "\n",
    "list_of_tuples = [('Peter', 60, 'M'), ('Thomas', 59, 'M'), ('Michael', 54, 'M'), ('Anabel', 50, 'F')]\n",
    "spark.createDataFrame(list_of_tuples).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf107e-2354-4eab-a067-e20c10b7cd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame from a list of tuples specifying column names\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes\").getOrCreate()\n",
    "\n",
    "list_of_tuples = [('Peter', 60, 'M'), ('Thomas', 59, 'M'), ('Michael', 54, 'M'), ('Anabel', 50, 'F')]\n",
    "spark.createDataFrame(list_of_tuples, ['Name', 'Age', 'Gender']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f04d6c-7e91-45c8-a36a-00db60c16301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame from a list of dictionaries (key-value-pairs)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes\").getOrCreate()\n",
    "\n",
    "list_of_dictionaries = [{'Name': 'Peter', 'Age':60, 'Gender': 'M'}, {'Name': 'Thomas', 'Age':59, 'Gender': 'M'}, {'Name': 'Michael', 'Age':54, 'Gender': 'M'}, {'Name': 'Anabel', 'Age':50, 'Gender': 'F'}]\n",
    "df = spark.createDataFrame(list_of_dictionaries).select('Name', 'Age', 'Gender')\n",
    "# df.show(2)\n",
    "# df.show(vertical = True)\n",
    "# df.show(2, vertical = True)\n",
    "df.filter(df.Gender != 'F').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e96294-fa18-45ee-8d1d-7d685d9b5555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with a schema inferred from the data\n",
    "\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes 2\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "\n",
    "print(df) # Schema is inferred\n",
    "print()\n",
    "\n",
    "for r in df.collect():\n",
    "   print(r[0], r[1], r[2], r[3], r[4] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c47d00b-5003-4f86-bf88-b12e8193973e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with an explicit schema\n",
    "\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes 3\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "\n",
    "# df.show()\n",
    "# df.show(1, vertical = True)\n",
    "\n",
    "# df.printSchema()\n",
    "# df.columns\n",
    "# df.select(\"a\", \"b\", \"c\").describe().show()\n",
    "# print(type(df.a))\n",
    "# df.select(df.c).show()\n",
    "# df.collect()\n",
    "# df.take(2)\n",
    "# df.filter(df.a == 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c466f-82c5-4732-b4c8-9621bdc67a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame aggregations\n",
    "\n",
    "from datetime import datetime, date\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes 4\").getOrCreate()\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "\n",
    "df.show()\n",
    "df.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f0204-44f3-4155-a51f-9ddadb318a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames: Processing data in CSV files \n",
    "\n",
    "spark = SparkSession.builder.appName(\"CSV\").getOrCreate()\n",
    "\n",
    "df_customers = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .load(\"/home/peter/Projects/spark/data/customers.csv\")\n",
    "\n",
    "# number of records in a dataframe\n",
    "# print(df_customers.count())\n",
    "\n",
    "# show first 10 records of a dataframe\n",
    "# df_customers.limit(10).show()\n",
    "\n",
    "# show selected columns only\n",
    "# df_customers.select('last_name', 'first_name', 'gender_code').limit(10).show()\n",
    "\n",
    "# filter by gender code\n",
    "# df_customers.select('last_name', 'first_name', 'gender_code').filter(\"gender_code == 'F'\").limit(10).show()\n",
    "\n",
    "# write file with female entries only\n",
    "# df_female_customers = df_customers.filter(\"gender_code == 'F'\")\n",
    "# df_female_customers.limit(10).show()\n",
    "# df_female_customers.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"|\").save(\"/home/peter/Projects/spark/data/female_customers.csv\")\n",
    "df2 = spark.read.format(\"csv\").option(\"inferSchema\", \"false\").option(\"header\", \"false\").option(\"sep\", \"|\").load(\"/home/peter/Projects/spark/data/female_customers.csv\")\n",
    "df2.limit(10).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "728d01a0-35a9-4f1a-a82b-f24047faf15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/20 11:55:58 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- birth_date: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- country_code: string (nullable = true)\n",
      " |-- customer_number: long (nullable = true)\n",
      " |-- data_date_part: string (nullable = true)\n",
      " |-- entity_id: long (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- gender_code: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- postal_code: long (nullable = true)\n",
      " |-- record_number: long (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- valid_from_date: string (nullable = true)\n",
      " |-- valid_to_date: string (nullable = true)\n",
      "\n",
      "+---------+----------+\n",
      "|last_name|first_name|\n",
      "+---------+----------+\n",
      "|Schönland| Willfried|\n",
      "|   Johann|Friedemann|\n",
      "|Christoph|      Hugo|\n",
      "|    Barth|    Moritz|\n",
      "|   Keudel|    Rouven|\n",
      "| Schuster|    Justus|\n",
      "|     Rose|    Rosita|\n",
      "|   Flantz|    Marian|\n",
      "|    Junck| Kornelius|\n",
      "|     Junk|    Hatice|\n",
      "+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+----------+-----------+\n",
      "|  last_name|first_name|gender_code|\n",
      "+-----------+----------+-----------+\n",
      "|       Rose|    Rosita|          F|\n",
      "|       Junk|    Hatice|          F|\n",
      "|     Biggen|  Virginia|          F|\n",
      "|      Gnatz|      Ella|          F|\n",
      "|Wagenknecht|  Reinhild|          F|\n",
      "|    Hauffer|  Birgitta|          F|\n",
      "|    Gerlach|    Sabina|          F|\n",
      "|  Striebitz|   Hiltrud|          F|\n",
      "|   Fröhlich|   Evelyne|          F|\n",
      "|     Hiller|   Damaris|          F|\n",
      "+-----------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+-----+\n",
      "|gender_code|count|\n",
      "+-----------+-----+\n",
      "|          F| 4980|\n",
      "|          M| 5020|\n",
      "+-----------+-----+\n",
      "\n",
      "+----------+----------+----------+\n",
      "|first_name| last_name|birth_date|\n",
      "+----------+----------+----------+\n",
      "|     Peter|   Langern|17.03.1994|\n",
      "|     Peter|Dussen van|03.06.1964|\n",
      "|     Peter|    Kobelt|20.05.1963|\n",
      "|     Peter|      Hein|13.12.1989|\n",
      "|     Peter|   Gerlach|25.07.1995|\n",
      "|     Peter|    Binner|14.11.1941|\n",
      "|     Peter|      Gieß|30.04.1963|\n",
      "|     Peter|      Roht|03.09.1988|\n",
      "+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrames: Playing around with JSON files\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Read a JSON File\").getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"multiline\",\"true\").json(\"/home/peter/Projects/spark/data/customers.json\")\n",
    "df.printSchema()\n",
    "\n",
    "# Display some columns of the firts 10 DataFrame records\n",
    "df.select('last_name', 'first_name').show(10)\n",
    "\n",
    "# Display females only\n",
    "df.select('last_name', 'first_name', 'gender_code').filter(df['gender_code'] == 'F').show(10)\n",
    "\n",
    "# Group by gender\n",
    "df.groupBy(\"gender_code\").count().show()\n",
    "\n",
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM customers where first_name =='Peter'\")\n",
    "sqlDF.select('first_name', 'last_name', 'birth_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79f95d-6a77-400d-b62b-178f958c3a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations - grouping. average, min, max\n",
    "\n",
    "# import non-standard functions\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Dataframes and aggretations\").getOrCreate()\n",
    "\n",
    "# create a dataframe with named columns from an array of tuples (corresponds to a table with 2 columns and 6 rows) \n",
    "person_df = spark.createDataFrame([(\"Peter\", 20), (\"Thomas\", 31), (\"Michael\", 30), (\"Anabel\", 35), (\"Elke\", 25), (\"Peter\", 58)], [\"name\", \"age\"])\n",
    "\n",
    "# group person with the same name, aggregate over ages and calculate avg age\n",
    "avg_age_df = person_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "\n",
    "# show results\n",
    "person_df.show()\n",
    "avg_age_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b710a5e2-7500-4fe7-b3f8-adbf45cb45cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/20 11:58:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------------+-----------------+----+-----------+--------------+\n",
      "|record_number|entity_id|customer_number|instalment_amount|term|debt_amount|data_date_part|\n",
      "+-------------+---------+---------------+-----------------+----+-----------+--------------+\n",
      "|           13|     3294|      000000000|           438.41|  32|   14029.12|    2021-11-08|\n",
      "|           19|     3296|      000000000|           297.86|  29|    8637.94|    2021-11-08|\n",
      "|          175|     3295|      000000000|           176.82|  17|    3005.94|    2021-11-08|\n",
      "|          410|     3296|      000000000|           418.59|  22|    9208.98|    2021-11-08|\n",
      "|          713|     3295|      000000000|           135.01|  35|    4725.35|    2021-11-08|\n",
      "|          905|     3294|      000000000|           381.41|  37|   14112.17|    2021-11-08|\n",
      "|          978|     3294|      000000000|           364.22|  27|    9833.94|    2021-11-08|\n",
      "|         1023|     3296|      000000000|           345.46|   2|     690.92|    2021-11-08|\n",
      "|         1126|     3293|      000000000|           191.34|   6|    1148.04|    2021-11-08|\n",
      "|         1281|     3295|      000000000|           190.21|  19|    3613.99|    2021-11-08|\n",
      "|         1310|     3294|      000000000|           385.78|  28|   10801.84|    2021-11-08|\n",
      "|         1441|     3293|      000000000|           187.09|  34|    6361.06|    2021-11-08|\n",
      "|         1472|     3294|      000000000|           419.77|   9|    3777.93|    2021-11-08|\n",
      "|         1475|     3295|      000000000|           403.73|  45|   18167.85|    2021-11-08|\n",
      "|         1700|     3296|      000000000|           204.97|  40|     8198.8|    2021-11-08|\n",
      "|         1905|     3293|      000000000|           180.15|  30|     5404.5|    2021-11-08|\n",
      "|         1966|     3296|      000000000|           390.29|  14|    5464.06|    2021-11-08|\n",
      "|         2181|     3294|      000000000|           436.31|  42|   18325.02|    2021-11-08|\n",
      "|         2209|     3295|      000000000|           479.41|  38|   18217.58|    2021-11-08|\n",
      "|         2378|     3296|      000000000|           466.64|  30|    13999.2|    2021-11-08|\n",
      "+-------------+---------+---------------+-----------------+----+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Playing around with RDDs and DataFrames\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDDs and DataFrames\").getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row\n",
    "rdd_customers_raw = sc.textFile(\"/home/peter/Projects/spark/data/customers.csv\")\n",
    "rdd_balances_raw = sc.textFile(\"/home/peter/Projects/spark/data/balances.csv\")\n",
    "\n",
    "rdd_customer_attributes = rdd_customers_raw.map(lambda l: l.split(\"|\"))\n",
    "rdd_balances_attributes = rdd_balances_raw.map(lambda l: l.split(\"|\"))\n",
    "\n",
    "rdd_customers = rdd_customer_attributes.map(lambda a: Row(record_number=a[0], \\\n",
    "                                         entity_id=a[1], \\\n",
    "                                         customer_number=a[2], \\\n",
    "                                         valid_from_date=a[3], \\\n",
    "                                         valid_to_date=a[4], \\\n",
    "                                         gender_code=a[5], \\\n",
    "                                         last_name=a[6], \\\n",
    "                                         first_name=a[7], \\\n",
    "                                         birth_date=a[8], \\\n",
    "                                         country_code=a[9], \\\n",
    "                                         postal_code=a[10], \\\n",
    "                                         city=a[11], \\\n",
    "                                         street=a[12], \\\n",
    "                                         data_date_part=a[13]))\n",
    "\n",
    "rdd_balances = rdd_balances_attributes.map(lambda a: Row(record_number=a[0], \\\n",
    "                                         entity_id=a[1], \\\n",
    "                                         customer_number=a[2], \\\n",
    "                                         instalment_amount=a[3], \\\n",
    "                                         term=a[4], \\\n",
    "                                         debt_amount=a[5], \\\n",
    "                                         data_date_part=a[6]))\n",
    "\n",
    "# Infer the schemas and register the DataFrames as a tables\n",
    "df_customers = spark.createDataFrame(rdd_customers)\n",
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "df_balances = spark.createDataFrame(rdd_balances)\n",
    "df_balances.createOrReplaceTempView(\"balances\")\n",
    "\n",
    "# Find balance records that have no customer\n",
    "df_balances_without_customer = spark.sql(\"SELECT balances.* FROM balances LEFT JOIN customers ON balances.customer_number = customers.customer_number WHERE customers.customer_number IS NULL;\")\n",
    "df_balances_without_customer.show()\n",
    "\n",
    "# females = spark.sql(\"SELECT last_name, first_name, gender_code FROM customers WHERE gender_code='F' LIMIT 10\")\n",
    "# female_names = females.rdd.map(lambda f: \"Name: \" + f.first_name).collect()\n",
    "\n",
    "# for name in female_names:\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f000af6-2d67-4be6-914f-ff13b5da764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apache Iceberg\n",
    "\n",
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"Iceberg\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/home/peter/Projects/spark/warehouse\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df_customers = spark.read.format(\"com.databricks.spark.csv\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"/home/peter/Projects/spark/data/customers.csv\")\n",
    "\n",
    "df_customers.select('last_name', 'first_name', 'gender_code', 'data_date_part').filter(\"gender_code = 'F'\").createOrReplaceTempView(\"temp_view_customers\")\n",
    "spark.sql(\"CREATE or REPLACE TABLE local.db.customers USING iceberg AS SELECT * FROM temp_view_customers\")\n",
    "df = spark.sql(\"SELECT last_name, first_name, data_date_part FROM local.db.customers\")\n",
    "df.show()\n",
    "df = spark.sql(\"UPDATE local.db.customers SET data_date_part = '2023-10-20' WHERE data_date_part = '2021-11-08'\")\n",
    "df = spark.sql(\"SELECT last_name, first_name, data_date_part FROM local.db.customers\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c3c19a7e-42f2-4781-9507-c232d2b16a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/20 12:00:55 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+\n",
      "|last_name|first_name|birth_date|\n",
      "+---------+----------+----------+\n",
      "|    Dobes|    Danuta|1998-01-23|\n",
      "|   Dippel|     Maike|1945-08-09|\n",
      "|     Pohl|      Anni|2002-08-10|\n",
      "|     Kade|  Siegbert|1977-03-24|\n",
      "|  Kühnert| Dietlinde|1971-04-09|\n",
      "|    Lange|   Baptist|1975-01-28|\n",
      "|   Hecker|     Elena|1965-05-30|\n",
      "|   Dowerg|   Aribert|1961-05-30|\n",
      "|    Heser|    Norman|1952-05-09|\n",
      "|   Ladeck|Karl-Josef|1989-08-18|\n",
      "+---------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a function to read from a MariaDB and from PostgreSQL database table\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Databases\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark-3.5.0/jars/postgresql-42.6.0.jar\") \\\n",
    "    .config(\"spark.jars\", \"/opt/spark-3.5.0/jars/mariadb-java-client-3.2.0.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def show_customers(spark: SparkSession, database) -> None:\n",
    "    if (database.lower() == \"mariadb\"):\n",
    "        df_customers = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:mysql://localhost:3306/shop?permitMysqlScheme\") \\\n",
    "            .option(\"driver\", \"org.mariadb.jdbc.Driver\") \\\n",
    "            .option(\"dbtable\", \"customers\") \\\n",
    "            .option(\"user\", \"spark\") \\\n",
    "            .option(\"password\", \"spark\") \\\n",
    "            .load()\n",
    "        df_customers.select('last_name', 'first_name', 'birth_date').show(10)\n",
    "    elif (database.lower() == \"postgresql\"):\n",
    "        df_customers = spark.read \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", \"jdbc:postgresql://localhost:5432/galeria_anatomica\") \\\n",
    "            .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "            .option(\"dbtable\", \"shop.customers\") \\\n",
    "            .option(\"user\", \"spark\") \\\n",
    "            .option(\"password\", \"spark\") \\\n",
    "            .load()\n",
    "        df_customers.select('last_name', 'first_name', 'birth_date').show(10)\n",
    "    else:\n",
    "        print(\"Unbekannter Datenbanktyp\")\n",
    "\n",
    "show_customers(spark, \"postgresql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b123ce14-b78e-4bb6-93df-94142db71fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write and read Parquet files\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Write and read a Parquet file\").getOrCreate()\n",
    "\n",
    "customers_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"sep\", \"|\").load(\"/home/peter/Projects/spark/data/customers.csv\")\n",
    "customers_df.write.mode(\"overwrite\").parquet(\"/home/peter/Projects/spark/data/customers.parquet\")\n",
    "parquet_df = spark.read.parquet(\"/home/peter/Projects/spark/data/customers.parquet\")\n",
    "parquet_df.createOrReplaceTempView(\"parquet_file\")\n",
    "males_df = spark.sql(\"SELECT last_name, first_name, gender_code FROM parquet_file WHERE gender_code = 'M'\")\n",
    "males_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad5ca7f-e05a-426b-852b-2f2e666585a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 12345) \\\n",
    "    .load()\n",
    "\n",
    "# Split the lines into words\n",
    "words = lines.select(\n",
    "   explode(\n",
    "       split(lines.value, \" \")\n",
    "   ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Generate running word count\n",
    "wordCounts = words.groupBy(\"word\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
